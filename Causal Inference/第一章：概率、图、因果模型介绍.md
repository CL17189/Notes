# 1.1 概率论
**这一节从为什么要引入概率讲起，介绍了概率的一些性质**
## 1.为什么研究因果分析的开始是概率：
观察因果的语境常常是“不确定的”；
自然语言更容易受到exception影响（因为颗粒度是有限的，概率论对一些不明确的例外有包容性。
但是也有局限性...
## 2.基本概念：主要是概率论和贝叶斯的一些公式；
+ 概率的基本性质...
## 3.定义了后验概率
O(H)，似然比L()
乘积公式：$O(H |e) =L(e | H)O(H).$
## 4.随机变量和期望：
离散和连续下的期望、方差的表达形式；一些记号
## 5.条件独立：
+ **Conditional Independence：** Let V ={ V1, V2,...V6} be a finite set of variables. Let P(.) be a joint probability function over the variables in V, and let X, Y, Z stand for any three subsets of variables in V. The sets X and Y are said to be conditionally independent given Z if P(x | y, z) = P(x | z) whenever P(y, z) = 0
相比primer那本书这里似乎是定义了一个拓展版本，因为这里X,Y,Z是随机变量集合而非单个变量。
紧接着，为了方便我们发明新记号：
-     (X$\perp \!\!\! \perp$Y|Z)P iff P(x | y, z) = P(x | z)
继续拓展，如果不需要任何条件那么Z取空集就行了，定义为：
(X $\perp \!\!\! \perp$Y |0) iff P(x | y) = P(x) whenever P(y) > 0
*  注意：定义的不可逆性。也就是说如果条件独立了那么对任意一对分别来自X和Y的随机变量他们也是条件独立的，但是反正不成立。反例很容易想到
* 那么这个符号，或者这个关系有什么性质呢？（证明略，写成概率形式用条件概率易得）
![[Pasted image 20230516221748.png]]
+ 公式还是太抽象了，说人话（具体）或者从直观上看是什么意思呢？作者这段话把图和概率公式结合了起来，巧妙地为下一节的开启做好了铺垫
（底下这段话的意思就是因果图，看过primer的读者可能一下会想到因果图以及作者有多爱因果图）
>The **symmetry axiom** states that, in any state of knowledge Z, if Y tells us nothing new about X, then X tells us nothing new about Y.
> The **decomposition axiom** asserts that if two combined items of information are judged irrelevant to X, then each separate item is irrelevant as well. 
> The weak union axiom states that learning irrelevant information W cannot help the irrelevant information Y become relevant to X. 
> The contraction axiom states that if we judge W irrelevant to X after learning some irrelevant information Y, then W must have been irrelevant before we learned Y. 
> Together, the weak union and contraction properties mean that irrelevant information should not alter the relevance status of other propositions in the system; what was relevant remains relevant, and what was irrelevant remains irrelevant. 
> The intersection axiom states that if Y is irrelevant to X when we know W and if W is irrelevant to X when we know Y, then neither W nor Y (nor their combination) is relevant to X.

**画画图就知道了！**

# 1.2 图和概率
## 1.记号和术语：
俗话说见人说人话，见鬼说鬼话，那么这一节介绍在图论中我们大致都会说些什么话。
+ 图的零部件：vertices (or nodes),edges (or links)；
+ 部件之间的关系：adjacent；
+ 有不同零件的图分别叫什么名字：directed,bidirected,；path, directed path; （a)cyclic; DAG;
+ 在有向图中我们怎么称呼两个节点之间的关系？——用kinship+ tree;
## 2.贝叶斯网络（图和概率的关系）
作者认为图的作用在统计和概率模型中有三个——假设、表示联合分布、做出推断。我们从第二个讲起。
由一个case 引入，如果我们要记录一个n元联合分布，组成这个联合分布的每一个一元分布都是个伯努利分布，那么要记录这个联合分布我们就需要$2^n$级别的空间，太大了！需要拆解，怎么拆解？
如果这些变量之间有依赖关系就好了。作者这里直接告诉我们，借助有向图和无向图我们都可以完成这种拆解。无向图(Undirected graphs) sometimes called Markov networks (Pearl 1988b), are used primarily to represent symmetrical spatial relationships；这个我们留到下一节介绍。而有向图Directed graphs, especially DAGs, have been used to represent causal or temporal relationships，and came to be known as Bayesian networks.
+ 怎么利用DAG拆解呢？
根据概率公式，不难写出：$P(x_1, ... , x_n) = \Pi_j P(xj | x_1, ... , xj_1).$
考虑更“实际”一些的情况，假设对于$x_j$来说，不一定所有的变量都和它有关，那么用一下我们上几节得到的条件独立，条件里边j-1个变量就可以精简成和$x_j$有关的变量，我们管它们叫做predecessors，用$pa_j$来表示这些变量的集合。写成公式也就是 $P(x_j| x_1... , x_j-1) = P(xj | pa_j).$ 上面我们描述的这件事由Markov定义出来了（可能是他做的吧我不知道具体是谁）
+ **Markovian Parents：** (筛除无关变量之后剩余的变量)Let V ={ X1, X2,...Xn} be an ordered set of variables, and let P(y) be the joint probability distribution on these variables. A set of variables PAj is said to be Markovian parents of Xj if PAj is a minimal set of predecessors of Xj that renders Xj independent of all its other predecessors. In other words, PAj is any subset of { X1, X2,...Xn} satisfying 
$$P(x_j| x_1... , x_j-1) = P(xj | pa_j)
$$    and such that no proper subset of PAj satisfies
关于这个定义很自然会产生一个问题？这个PAj是唯一的吗？
>推论：It can be shown (Pearl 1988b) that the set PAj is unique whenever the distribution P(y) is strictly positive (i.e., involving no logical or definitional constraints), so that every configuration y of variables, no matter how unlikely, has some finite probability of occurring. Under such conditions, the Bayesian network associated with P(y) is unique, given the ordering of the variables.

上面的定义利用条件独立为贝叶斯网络的构建提供了桥梁，但是我们发现对于总体的联合分布在经过这个分解之后我们不用再考虑变量的顺序了。因此，我们得出结论，DAG G成为概率分布P的贝叶斯网络的一个必要条件是，P承认G所规定的乘积分解
+ **Markov Compatibility：** If a probability function P admits the factorization of $P(x_1, ... , x_n) = \Pi_j P(xj | pa_j).$ relative to DAG G, we say that G represents P, that G and P are compatible, or that P is Markov relative to G
+ 一对充要条件: 按$P_i (xj | pa_j)$ 随机抽取<=>Markov relative
=>如何列出这些$P_i (xj | pa_j)$？需要独立性，独立性如何检测？=>d-seperation

## 3.d-seperation：
实质上就是几种图的情况
（可以先从primer上看一下什么是chain,folk,collider这里直接放出来了)
+ d-Separation：A path p is said to be d-separated (or blocked) by a set of nodes Z if and only if
1. p contains a chain i->m->j or a fork i<-m->j such that the middle node m is in Z, or 
2. p contains an inverted fork (or collider) i->m<-j such that the middle node m is not in Z and such that no descendant of m is in Z. 
A set Z is said to d-separate X from Y if and only if Z blocks every path from a node in X to a node in Y.
*->a case about colider: selection bias or Berkson’s paradox*
那么引入了d-seperation之后和条件独立有什么关系呢？
+ **TH:** If sets X and Y are d-separated by Z in a DAG G, then X is independent of Y conditional on Z in every distribution compatible with G. Conversely, if X and Y are not d-separated by Z in a DAG G, then X and Y are dependent conditional on Z in at least one distribution compatible with G
说明d-seperation 条件独立的关系,把这两种关系有时记作：$$(X\perp \!\!\! \perp Y|Z)_P 和 (X\perp \!\!\! \perp Y|Z)_G$$
上面的定理换个写法就是：![[Pasted image 20230517210509.png]]
生成图的顺序在d-准则里面没有被考虑，因为只有生成图的拓扑结构决定P要满足的独立集，如下面定理：
+ **Ordered Markov Condition：** A necessary and sufficient condition for a probability distribution P to be Markov relative a DAG G is that, conditional on its parents in G, each variable be independent of all its predecessors in some ordering of the variables that agrees with the arrows of G.
（和排序是独立的）
这个定理可以用来决定一个P是否和G是Markov relative的（因为是充要条件嘛），也就是说
+ **Parental Markov Condition：** A necessary and sufficient condition for a probability distribution P to be Markov relative a DAG G is that every variable be independent of all its nondescendants (in G), conditional on its parents. (We exclude $X_i$ when speaking of its “nondescendants.”)
作者说有人用这个定理来定义bayesian网络，但是事实上上一个定理更好用（我猜可能是因为箭头更直观）
考虑完一个P和一个G的关系，我们考虑一个P和多个G的关系。也就是说对于同一个P而言，它和一个图是compatible的，那对于什么类型的图也是compatible的呢？
+ **Observational Equivalence：** Two DAGs are observationally equivalent if and only if they have the same skeletons and the same sets of v-structures, that is, two converging arrows whose tails are not connected by an arrow (Verma and Pearl 1990

## 4. Bayes网络推断
这一小节讲了从贝叶斯公式开始那个条件概率确实可以算出来，但是challenge-是要computations efficiently and within the representation level provided by the network topology.
于是有了一些算法Algorithms->trees->cut-set conditioning method


# 1.3 因果贝叶斯网络
作者描述为a more natural and more reliable way of expressing what we know or believe about the world since conditional independence judgments are by-products of stored causal relationships
因为它具有**the ability to represent and respond to external or spontaneous changes.**

## 1. 网络对于干预
我们希望它有灵活性：调整或者假定一个变量的状态，直接删去指向它的箭头在概率公式里面加入那个状态。在这个改变之后我们希望这个东西还可以接着用。于是：
这里没有介绍的直接加入了do算子，啊作者真是假定他的读者都看过primer
![[Pasted image 20230517213054.png]]
![[Pasted image 20230517213103.png]]
接下来不加证明的给出了两个性质：
![[Pasted image 20230517213302.png]]
作者贴心的告诉我们性质一控制了父集合相对于子集是外生的，保证干预和加入条件的效果一样；性质二则告诉我们控制了原因（父节点），再加入任何的措施都是没有效果的。这都是符合直觉的。
>Property 1 renders every parent set PAi exogenous relative to its child Vi, ensuring that the conditional probability coincides with the effect (on Vi) of setting PAi to pai by external control. Property 2 expresses the notion of invariance; once we control its direct causes PAi, no other interventions will affect the probability of Vi.


## 2.因果关系和稳定性
ontological(C)&epistemic(P)；
in marked contrast to probabilistic relationships, causal relationships remain invariant to changes in the mechanism that governs the causal variables；
->whatever judgments people express about conditional independencies in a given domain are derived from the causal structure acquired.
->causal models need not encode behavior under intervention but instead aim primarily to provide an “explanation” or “understanding” of how data are generated.
没看太懂且很长，让我想起了关于GRE的一些不好的回忆  :(

# 1.4 FUNCTIONAL CAUSAL MODELS

引入：和Bayes的想法不同的是，一些经济、社会学家引入关于不可见变量的assumption，打算用deterministic, functional equations, and probabilities解决因果问题。这个想法的哲学基础来自Laplace。作者认为L的quasi-deterministic这个想法比随机概念要好，主要因为三点：更一般（没有那么多的函数模拟）、更符合人类直觉、更符合人类广泛存在的一些概念（这和反事实有关）

## 1.结构方程：
+ functional causal model： $$x_i=f_i(pa_i,u_i),i=1,...n$$
从经济学的供给价格模型出发作为例子->对比f是线性、非线性时候函数的情况
->和贝叶斯网络的对比，我们要考虑三个问题：预测、干预、反事实

## 2. 概率预测
首先引入一些定义：因果图（causal diagram）是根据functional causal model连好箭头的图 ；如果它没有环我们叫它是semi-Markovian的，它的模型的X值完全取决于U，P(X)<=>P(u),；如果有环 ,我们就叫它是Markovian的。
那么这个模型就可以通过之前我们讨论过的parental Markov condition和概率还有因果联系起来
![[Pasted image 20230517215932.png]]
 就算是模型中的每个X都之和非后代的节点有关。
 而且这个定理告诉我们之前的parental Markov condition包含了两个因果假设：
 1. every variable that is a cause of two or more other variables
 2. if any two variables are dependent, then one is a cause of the other or there is a third variable causing both.
causal Markov condition用确定的函数表示独立性和bayes用条件概率在分解上是等价的.
但是causal–functional specification has several advantages over the probabilistic specification)
->these advantages are...不列举了

## 3. 干预和因果效应
all features of causal Bayesian networks (Section 1.3) can be emulated in Markovian functional models.
offers greater flexibility and generality than that of a stochastic model
->具体做法是把那个变量换成常量

## 4.反事实
反事实的部分是利用因果网络做一些统计推断不能办到的检验
在一个例子之后，我们得到了几个教训：
The first lesson of this example is that stochastic causal models are insufficient for computing probabilities of counterfactuals; knowledge of the actual process behind P(y ƒ x) is needed for the computation.24 A second lesson is that a functional causal model constitutes a mathematical object sufficient for the computation (and definition) of such probabilities.
所以提出了做法，要解决的问题（公式化）
![[Pasted image 20230517221243.png]]

生成因果模型的三步
# 1.5 CAUSAL VERSUS STATISTICAL TERMINOLOGY
第五节其实就是区分一些本书中用到的话术，主要分为
概率&统计&因果 三个维度讨论


# summary:
+ This hierarchy also defines a natural partitioning of the chapters in this book. Chapter 2 will deal primarily with the probabilistic aspects of causal Bayesian networks (though the underlying causal structure will serve as a conceptual guide). Chapters 3–6 will deal exclusively with the interventional aspects of causal models, including the identification of causal effects, the clarification of structural equation models, and the relationships between confounding and collapsibility. Chapters 7–10 will deal with counterfactual analysis, including axiomatic foundation, applications to policy analysis, the bounding of counterfactual queries, the identification of probabilities of causes, and the explication of single-event causation.
+ 总结：第一章从概率引入了因果，讲明概率在因果推断里的重要性，随即补充一些概率知识。然后把概率、和图用bayes网络连接起来，重点介绍贝叶斯网路，中间穿插因果推断的概念。
+ 第三节接着加入因果，讲贝叶斯因果网络的概念；第四节引入更一般的结构方程，交代我们在因果推断里面用结构方程要做的三件事：预测、干预、反事实；后续的三个小节是对这三件事的小展开（主要讲引出的逻辑），结合前面的例子，分析markov模型和贝叶斯模型的不同，同时时不时提到后文内容。
+  最后这一章作为本书的介绍章，强调书中的概率&统计&因果概念的差别。